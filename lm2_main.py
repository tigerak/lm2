import os
import sys
sys.path.append(os.path.abspath(os.path.join(__file__, "../../..")))

import numpy as np
import json
from time import time

import torch
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast

# huggingface
from transformers import (Trainer, TrainingArguments,
                          AutoTokenizer, AutoModelForCausalLM, 
                          LlamaConfig,
                          BitsAndBytesConfig)

from config import *
from function.lm2.lm2_memory_module import LM2MemoryModule
from function.lm2.lm2_causal import LM2ForCausalLM
from function.lm2.lm2_dataset import LM2_Dataset, collate_fn


if __name__=="__main__":

    use_bf16 = True

    lm2_config= LlamaConfig.from_pretrained(LLAMA3_2_KO_3B)
    # print(lm2_config)
    lm2_config.hidden_size = 256
    lm2_config.num_attention_heads = 4
    lm2_config.num_hidden_layers = 16
    lm2_config.num_key_value_heads = 4
    
    lm2_config.num_slots = 4
    lm2_config.memory_rank = 64
    print(lm2_config)

    batch_size = 1
    accum_steps = 1
    total_epoch = 3

    d_model = lm2_config.hidden_size
    num_slots = lm2_config.num_slots
    memory_rank = lm2_config.memory_rank
    memory_module = LM2MemoryModule(d_model=d_model,
                                    num_slots=num_slots,
                                    memory_rank=memory_rank)
    

    model = LM2ForCausalLM(config=lm2_config,
                           memory_module=memory_module).cuda()
    if use_bf16:
        model = model.to(dtype=torch.bfloat16)
    
    tokenizer = AutoTokenizer.from_pretrained(LLAMA3_2_KO_3B)

    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    
    ### ê¸°ì¡´ í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ###
    leaning_model_path = save_dir + r'last_model.pt'
    start_epoch = 0
    new_mem = None
    if os.path.exists(leaning_model_path):
        print(f"ê¸°ì¡´ í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì‹œì‘: {leaning_model_path}")

        checkpoint = torch.load(leaning_model_path, map_location="cpu")
        model.load_state_dict(checkpoint["model_state_dict"], strict=False)
        if use_bf16:
            model = model.to(dtype=torch.bfloat16)
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        start_epoch = checkpoint["epoch"] 
        start_step = checkpoint["step"]
        new_mem = checkpoint["memory_states"]
        model.to("cuda")
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! {start_epoch}epoch ë¶€í„° í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.")
    else:
        print(f"ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.")
        start_step = 0

    print("ëª¨ë¸ ì¤€ë¹„ !!!!!!")
    
    
    # ë°ì´í„° ìƒì„±ì„±
    dataset = LM2_Dataset(json_path=YTN_DATA, 
                            tokenizer=tokenizer)
    
    dataloader = DataLoader(dataset,
                            batch_size=batch_size,
                            shuffle=True,
                            pin_memory=True,
                            collate_fn=collate_fn)

    # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    generation_results = []
    
    ### í•™ìŠµ ì‹œì‘ ###
    step_epoch = len(dataloader)
    total_steps = step_epoch/accum_steps
    print(f"ì´ ë°ì´í„° ìˆ˜: {dataset.__len__()} | ì‹¤ì œ ìŠ¤í…: {step_epoch} | ë°°ì¹˜ í•©ì‚°: {accum_steps}")
    for epoch in range(start_epoch, total_epoch):
        model.train()
        
        current_step = 0
        for step, batch in enumerate(dataloader):
            if start_step:
                current_step = (step + start_step)
            else:
                current_step = step
            
            start_time = time()

            input_ids = batch["input_ids"].cuda()
            attention_mask = batch["attention_mask"].cuda()
            labels = batch["labels"].cuda()
            
            logit, loss, new_mem = model(input_ids=input_ids,
                                        attention_mask=attention_mask,
                                        labels=labels,
                                        memory_states=new_mem) ####
            # new_mem = new_mem.detach() # Autograd ì¶”ì  ì°¨ë‹¨
            loss = loss / accum_steps
            if loss < 1e-6:
                loss = loss = loss.clamp(min=1e-6)
            loss.backward()
            # print(current_step)
            if (current_step + 1) % accum_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient í­ë°œ ë°©ì§€
                optimizer.step()
                optimizer.zero_grad()
                
                end_time = time()

                for i in range(10):
                    i += 30
                    input_word = tokenizer.decode(input_ids[0, i].item(), skip_special_tokens=False)
                    target_word = tokenizer.decode(labels[0, i+1].item(), skip_special_tokens=False) if labels[0, i].item() != -100 else "[IGNORE]"
        
                    predicted_token_id = logit[0, i].argmax(dim=-1).item()
                    predicted_word  = tokenizer.decode(predicted_token_id, skip_special_tokens=False)
                    
                    print(f"ğŸ”¹ ì…ë ¥: '{input_word}' | ì •ë‹µ: '{target_word}' | ì˜ˆì¸¡: '{predicted_word }'")

                print(f"[Epoch:{epoch} Step:{current_step//accum_steps}/{total_steps}] loss={loss.item():.4f} time:{end_time-start_time:.2f}")

                # # ğŸ”¹ ëª¨ë¸ ì €ì¥ (ê° stepë§ˆë‹¤ ì €ì¥)
                # model_save_path = os.path.join(save_dir, f"last_model.pt")
                # torch.save({
                #     "epoch": epoch,
                #     "step": current_step + 1,
                #     "model_state_dict": model.state_dict(),
                #     "optimizer_state_dict": optimizer.state_dict(),
                #     "memory_states": new_mem, ####
                # }, model_save_path)
                
                # print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")

            if step_epoch - current_step < accum_steps:
                leftover = step_epoch % accum_steps
                if leftover != 0:
                    optimizer.step()
                    optimizer.zero_grad()

            # if step == 2:
            #     break

        

        model.eval()

        # test_text = "í•œêµ­ì „ë ¥ì´ ì‡ë”°ë¥¸ ì „ê¸°ìš”ê¸ˆ ì¸ìƒê³¼ êµ­ì œ ì—°ë£Œë¹„ ì¸í•˜ íš¨ê³¼ ë•ë¶„ì— ì§€ë‚œí•´ 4ë…„ ë§Œì— í‘ì ì „í™˜ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤. í•œì „ì€ ì—°ê²° ê¸°ì¤€ ì§€ë‚œí•´ ì˜ì—…ì´ìµì„ 8ì¡° 3ì²œì–µ ì›ìœ¼ë¡œ ì ì • ì§‘ê³„í–ˆìŠµë‹ˆë‹¤. ì´ì— ë”°ë¼ 2021ë…„ ì´í›„ í•œì „ì˜ ëˆ„ì  ì˜ì—…ì ìëŠ” 34ì¡° 7ì²œì–µ ì›ìœ¼ë¡œ ì¤„ì—ˆìŠµë‹ˆë‹¤."
        
        test_title = "### TITLE: í•œì „, ì§€ë‚œí•´ ì˜ì—…ìµ 8ì¡° 3ì²œì–µ...4ë…„ ë§Œì— í‘ì ì „í™˜\n###ARTICLE: "
        encoding = tokenizer(text=test_title, 
                            truncation=True,
                            max_length=1024,
                            add_special_tokens=True,
                            return_tensors="pt").to("cuda") 
        input_ids = encoding["input_ids"]

        correct_predictions = 0
        total_predictions = 0


        generated = input_ids.clone()
        memory_states = new_mem #####
        for i in range(100):
            with torch.no_grad():
                out_logits, _, memory_states = model(
                    input_ids=generated,
                    attention_mask=torch.ones_like(generated).to("cuda"), # forë¬¸ìœ¼ë¡œëŠ” í•„ìš”ìš”
                    memory_states=memory_states
                )
            next_token = out_logits[:, -1, :].argmax(dim=-1, keepdim=True)
            generated = torch.cat([generated, next_token], dim=-1)

        generated_text = "generated:", tokenizer.decode(generated[0].tolist(), skip_special_tokens=False)

        print(f"\n[Epoch {epoch+1}] ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
        print(f"ì…ë ¥: {test_title}")
        print(f"ì¶œë ¥: {generated_text}")

        # ğŸ”¹ ê²°ê³¼ ì €ì¥
        generation_results.append({
            "epoch": epoch + 1,
            "input": test_title,
            "generated": generated_text
        })

        # ğŸ”¹ ëª¨ë¸ ì €ì¥ (ê° epochë§ˆë‹¤ ì €ì¥)
        model_save_path = os.path.join(save_dir, f"last_model.pt")
        # model_save_path = os.path.join(save_dir, f"model_epoch_{epoch+1}.pt")
        torch.save({
            "epoch": epoch + 1,
            "step": 0,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "memory_states": new_mem, #####
        }, model_save_path)
        
        print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")

        # ğŸ”¹ ìµœì¢… ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        results_path = os.path.join(save_dir, "generation_results.json")
        with open(results_path, "w", encoding="utf-8") as f:
            json.dump(generation_results, f, ensure_ascii=False, indent=4)
            # tokenizer = AutoTokenizer.from_pretrained(LLAMA3_2_KO_3B)
    