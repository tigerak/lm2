import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import json
from time import time

import torch
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast

# huggingface
from transformers import (Trainer, TrainingArguments,
                          AutoTokenizer, AutoModelForCausalLM, 
                          LlamaConfig,
                          BitsAndBytesConfig)

from config import *
from function.lm2.memory_module import LM2MemoryModule
from function.lm2.lm2causal import LM2ForCausalLM
from function.lm2.lm2_dataset import LM2_Dataset, collate_fn


if __name__=="__main__":

    use_bf16 = True

    lm2_config= LlamaConfig.from_pretrained(LLAMA3_2_KO_3B)
    # print(lm2_config)
    lm2_config.hidden_size = 256
    lm2_config.num_attention_heads = 4
    lm2_config.num_hidden_layers = 16
    lm2_config.num_key_value_heads = 4
    
    lm2_config.num_slots = 8
    print(lm2_config)

    d_model = lm2_config.hidden_size
    num_slots = lm2_config.num_slots
    memory_module = LM2MemoryModule(d_model=d_model,
                                    num_slots=num_slots)
    

    model = LM2ForCausalLM(config=lm2_config,
                           memory_module=memory_module).cuda()
    if use_bf16:
        model = model.to(dtype=torch.bfloat16)
    
    tokenizer = AutoTokenizer.from_pretrained('beomi/OPEN-SOLAR-KO-10.7B')

    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    
    ### ê¸°ì¡´ í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ###
    leaning_model_path = r'/home/lab_ai/Project/output/last_model.pt'
    start_epoch = 0
    new_mem = None
    if os.path.exists(leaning_model_path):
        print(f"ê¸°ì¡´ í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì‹œì‘: {leaning_model_path}")

        checkpoint = torch.load(leaning_model_path, map_location="cpu")
        model.load_state_dict(checkpoint["model_state_dict"], strict=False)
        if use_bf16:
            model = model.to(dtype=torch.bfloat16)
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        start_epoch = checkpoint["epoch"] 
        start_step = checkpoint["step"]
        new_mem = checkpoint["memory_states"]
        model.to("cuda")
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! {start_epoch}epoch ë¶€í„° í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.")
    else:
        print(f"ğŸš¨ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.")
        start_step = 0

    print("ëª¨ë¸ ì¤€ë¹„ !!!!!!")
    
    
    
    # ë°ì´í„° ìƒì„±ì„±
    dataset = LM2_Dataset(json_path=YTN_DATA, 
                            tokenizer=tokenizer)
    
    dataloader = DataLoader(dataset,
                            batch_size=1,
                            shuffle=True,
                            pin_memory=True,
                            collate_fn=collate_fn)
    
    # training_args = TrainingArguments(
    #                         output_dir="./output",  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    #                         # evaluation_strategy="epoch",  # ë§¤ epochë§ˆë‹¤ ê²€ì¦ ì‹¤í–‰
    #                         save_strategy="epoch",  # ë§¤ epochë§ˆë‹¤ ëª¨ë¸ ì €ì¥
    #                         per_device_train_batch_size=1,  # GPUë‹¹ ë°°ì¹˜ í¬ê¸°
    #                         # per_device_eval_batch_size=1,
    #                         gradient_accumulation_steps=64,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ gradient ëˆ„ì 
    #                         learning_rate=5e-5,  # í•™ìŠµë¥ 
    #                         weight_decay=0.01,  # ê°€ì¤‘ì¹˜ ê°ì‡  (AdamW)
    #                         num_train_epochs=3,  # í•™ìŠµí•  epoch ìˆ˜
    #                         save_total_limit=2,  # ì €ì¥í•  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜ ì œí•œ
    #                         logging_dir="./output/logs",  # ë¡œê·¸ ì €ì¥ ê²½ë¡œ
    #                         logging_steps=1,  # 10 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    #                         # fp16=True,  # mixed precision (ì†ë„ í–¥ìƒ)
    #                         bf16=True,
    #                         push_to_hub=False,  # í•™ìŠµ ì™„ë£Œ í›„ í—ˆë¸Œì— ì—…ë¡œë“œ ì—¬ë¶€
    #                     )
    
    # trainer = Trainer(model=model,
    #                     args=training_args,
    #                     train_dataset=dataset)
    
    # trainer.train()

    # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    generation_results = []
    
    ### í•™ìŠµ ì‹œì‘ ###
    total_epoch = 3
    accum_steps = 8
    step_epoch = len(dataloader)
    total_steps = step_epoch/accum_steps
    print(f"ì´ ë°ì´í„° ìˆ˜: {dataset.__len__()} | ì‹¤ì œ ë°°ì¹˜: {step_epoch} | ë°°ì¹˜ í•©ì‚°: {accum_steps}")
    for epoch in range(start_epoch, total_epoch):
        model.train()
        
        current_step = 0
        for step, batch in enumerate(dataloader):
            if start_step:
                current_step = (step + start_step)
            else:
                current_step = step
            display_step = current_step // accum_steps
            start_time = time()

            input_ids = batch["input_ids"].cuda()
            attention_mask = batch["attention_mask"].cuda()
            labels = batch["labels"].cuda()
            
            logit, loss, new_mem = model(input_ids=input_ids,
                                        attention_mask=attention_mask,
                                        labels=labels,
                                        memory_states=None)
            # new_mem = new_mem.detach() # Autograd ì¶”ì  ì°¨ë‹¨
            loss = loss / accum_steps
            if loss < 1e-6:
                loss = torch.tensor(1e-6, requires_grad=True).to(loss.device)
            loss.backward()
            if (step + 1) % accum_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient í­ë°œ ë°©ì§€
                optimizer.step()
                optimizer.zero_grad()
                
                end_time = time()

                for i in range(5):
                    i += 30
                    input_word = tokenizer.decode(input_ids[0, i].item(), skip_special_tokens=False)
                    target_word = tokenizer.decode(labels[0, i+1].item(), skip_special_tokens=False) if labels[0, i].item() != -100 else "[IGNORE]"
        
                    predicted_token_id = logit[0, i].argmax(dim=-1).item()
                    predicted_word  = tokenizer.decode(predicted_token_id, skip_special_tokens=False)
                    
                    print(f"ğŸ”¹ ì…ë ¥: '{input_word}' | ì •ë‹µ: '{target_word}' | ì˜ˆì¸¡: '{predicted_word }'")

                print(f"[Epoch:{epoch} Step:{display_step}/{total_steps}] loss={loss.item():.4f} time:{end_time-start_time:.2f}")

            # if step == 3:
            #     break

            # ğŸ”¹ ëª¨ë¸ ì €ì¥ (ê° stepë§ˆë‹¤ ì €ì¥)
            model_save_path = os.path.join(save_dir, f"last_model.pt")
            torch.save({
                "epoch": epoch,
                "step": current_step + 1,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "memory_states": new_mem, ####
            }, model_save_path)
            
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")

            if total_steps - display_step < 1:
                break

        leftover = step_epoch % accum_steps
        if leftover != 0:
            optimizer.step()
            optimizer.zero_grad()

        model.eval()

        # test_text = "í•œêµ­ì „ë ¥ì´ ì‡ë”°ë¥¸ ì „ê¸°ìš”ê¸ˆ ì¸ìƒê³¼ êµ­ì œ ì—°ë£Œë¹„ ì¸í•˜ íš¨ê³¼ ë•ë¶„ì— ì§€ë‚œí•´ 4ë…„ ë§Œì— í‘ì ì „í™˜ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤. í•œì „ì€ ì—°ê²° ê¸°ì¤€ ì§€ë‚œí•´ ì˜ì—…ì´ìµì„ 8ì¡° 3ì²œì–µ ì›ìœ¼ë¡œ ì ì • ì§‘ê³„í–ˆìŠµë‹ˆë‹¤. ì´ì— ë”°ë¼ 2021ë…„ ì´í›„ í•œì „ì˜ ëˆ„ì  ì˜ì—…ì ìëŠ” 34ì¡° 7ì²œì–µ ì›ìœ¼ë¡œ ì¤„ì—ˆìŠµë‹ˆë‹¤."
        
        test_title = "### TITLE: í•œì „, ì§€ë‚œí•´ ì˜ì—…ìµ 8ì¡° 3ì²œì–µ...4ë…„ ë§Œì— í‘ì ì „í™˜\n###ARTICLE: "
        encoding = tokenizer(text=test_title, 
                            truncation=True,
                            max_length=1024,
                            add_special_tokens=True,
                            return_tensors="pt").to("cuda") 
        input_ids = encoding["input_ids"]

        correct_predictions = 0
        total_predictions = 0

        # with torch.no_grad():
        #     output = model.generate(input_ids, max_new_tokens=100, do_sample=False)

        # generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        # print(f"\nğŸ”¹ [Epoch {epoch+1}] ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
        # print(f"ì…ë ¥: {test_text}")
        # print(f"ì¶œë ¥: {generated_text}")

        generated = input_ids.clone()
        memory_states = new_mem #########
        for i in range(100):
            with torch.no_grad():
                out_logits, _, memory_states = model(
                    input_ids=generated,
                    attention_mask=torch.ones_like(generated).to("cuda"), # forë¬¸ìœ¼ë¡œëŠ” í•„ìš”ìš”
                    memory_states=memory_states
                )
            next_token = out_logits[:, -1, :].argmax(dim=-1, keepdim=True)
            generated = torch.cat([generated, next_token], dim=-1)

        generated_text = "generated:", tokenizer.decode(generated[0].tolist(), skip_special_tokens=False)

        print(f"\nğŸ”¹ [Epoch {epoch+1}] ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
        print(f"ì…ë ¥: {test_title}")
        print(f"ì¶œë ¥: {generated_text}")

        # ğŸ”¹ ê²°ê³¼ ì €ì¥
        generation_results.append({
            "epoch": epoch + 1,
            "input": test_title,
            "generated": generated_text
        })

        # ğŸ”¹ ëª¨ë¸ ì €ì¥ (ê° epochë§ˆë‹¤ ì €ì¥)
        model_save_path = os.path.join(save_dir, f"last_model.pt")
        # model_save_path = os.path.join(save_dir, f"model_epoch_{epoch+1}.pt")
        torch.save({
            "epoch": epoch + 1,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "memory_states": new_mem, ######################
        }, model_save_path)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")

        # ğŸ”¹ ìµœì¢… ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        results_path = os.path.join(save_dir, "generation_results.json")
        with open(results_path, "w", encoding="utf-8") as f:
            json.dump(generation_results, f, ensure_ascii=False, indent=4)
            # tokenizer = AutoTokenizer.from_pretrained(LLAMA3_2_KO_3B)
    